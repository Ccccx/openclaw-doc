#!/usr/bin/env python3
"""
è·å–GitHub Trendingä¿¡æ¯çš„è„šæœ¬ - æ”¹è¿›ç‰ˆ
"""
import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime

def get_github_trending(since='daily'):
    """è·å–GitHub trendingé¡µé¢"""
    url = f"https://github.com/trending?since={since}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching trending: {e}")
        return None

def parse_trending_html(html_content):
    """è§£æHTMLè·å–trendingé¡¹ç›®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    repos = []
    
    # æŸ¥æ‰¾æ‰€æœ‰repoæ¡ç›®
    for article in soup.find_all('article', class_='Box-row'):
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯sponsors
            if article.find('span', class_='Label-sponsor'):
                continue
            
            # è·å–repoé“¾æ¥ - æŸ¥æ‰¾åŒ…å«é¡¹ç›®åçš„é“¾æ¥
            link_elem = article.find('a', href=re.compile(r'^/[a-zA-Z0-9-]+/[a-zA-Z0-9-._]+$'))
            if not link_elem:
                continue
            
            href = link_elem['href']
            if not href or '/sponsors' in href:
                continue
            
            # è§£æä½œè€…/é¡¹ç›®å
            parts = href.strip('/').split('/')
            if len(parts) < 2:
                continue
            
            author = parts[0]
            repo_name = parts[1]
            
            # è·å–staræ•° - æŸ¥æ‰¾åŒ…å«stargazersçš„é“¾æ¥
            star_elem = article.find('a', href=re.compile(r'/stargazers'))
            stars = star_elem.get_text(strip=True) if star_elem else "0"
            
            # è·å–ä»Šæ—¥å¢é•¿ - æŸ¥æ‰¾åŒ…å«ä»Šæ—¥æ•°æ®çš„span
            today_elem = article.find('span', class_=re.compile(r'd-\S+'))
            today_stars = today_elem.get_text(strip=True) if today_elem else ""
            
            # è·å–æè¿°
            desc_elem = article.find('p', class_='color-fg-muted')
            description = desc_elem.get_text(strip=True) if desc_elem else ""
            
            # è·å–è¯­è¨€
            lang_elem = article.find('span', itemprop='programmingLanguage')
            language = lang_elem.get_text(strip=True) if lang_elem else "Unknown"
            
            repos.append({
                'author': author,
                'repo': repo_name,
                'full_name': f"{author}/{repo_name}",
                'url': f"https://github.com{href}",
                'stars': stars,
                'today_stars': today_stars,
                'description': description,
                'language': language
            })
        except Exception as e:
            print(f"Error parsing repo: {e}")
            continue
    
    return repos

def main():
    print("æ­£åœ¨è·å–GitHub Trendingä¿¡æ¯...")
    html = get_github_trending('daily')
    
    if not html:
        print("è·å–å¤±è´¥")
        return
    
    # ä¿å­˜HTMLä¾›è°ƒè¯•
    with open('/tmp/github_trending.html', 'w') as f:
        f.write(html)
    
    repos = parse_trending_html(html)
    
    print(f"\nè·å–åˆ° {len(repos)} ä¸ªé¡¹ç›®:\n")
    for i, repo in enumerate(repos[:10], 1):
        print(f"{i}. {repo['full_name']}")
        print(f"   â­ï¸ {repo['stars']} (ä»Šæ—¥+{repo['today_stars']})")
        print(f"   ğŸ’» {repo['language']}")
        print(f"   ğŸ“ {repo['description'][:100]}...")
        print()
    
    # è¾“å‡ºJSONä¾›åç»­å¤„ç†
    print("\n=== JSON Output ===")
    print(json.dumps(repos[:10], indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()
